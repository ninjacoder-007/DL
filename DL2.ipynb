{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Describe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa85f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Biological neurons, consisting of a cell body, axons, dendrites and synapses, are able to process and transmit neural activation.\n",
    "\n",
    "image.png\n",
    "\n",
    "Biological neuron models, also known as a spiking neuron models,[1] are mathematical descriptions of the properties of certain cells in the nervous system that generate sharp electrical potentials across their cell membrane, roughly one millisecond in duration, called action potentials or spikes (Fig. 2). Since spikes are transmitted along the axon and synapses from the sending neuron to many other neurons, spiking neurons are considered to be a major information processing unit of the nervous system. Spiking neuron models can be divided into different categories: the most detailed mathematical models are biophysical neuron models (also called Hodgkin-Huxley models) that describe the membrane voltage as a function of the input current and the activation of ion channels. Mathematically simpler are integrate-and-fire models that describe the membrane voltage as a function of the input current and predict the spike times without a description of the biophysical processes that shape the time course of an action potential. Even more abstract models only predict output spikes (but not membrane voltage) as a function of the stimulation where the stimulation can occur through sensory input or pharmacologically. This article provides a short overview of different spiking neuron models and links, whenever possible to experimental phenomena. It includes deterministic and probabilistics models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f677bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7aee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigmoid / Logistic Activation Function\n",
    "This function takes any real value as input and outputs values in the range of 0 to 1.\n",
    "\n",
    "The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0, as shown below.\n",
    "\n",
    "Tanh Function (Hyperbolic Tangent)\n",
    "Tanh function is very similar to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.\n",
    "\n",
    "ReLU Function\n",
    "ReLU stands for Rectified Linear Unit.\n",
    "\n",
    "Although it gives an impression of a linear function, ReLU has a derivative function and allows for backpropagation while simultaneously making it computationally efficient.\n",
    "\n",
    "The main catch here is that the ReLU function does not activate all the neurons at the same time.\n",
    "\n",
    "The neurons will only be deactivated if the output of the linear transformation is less than 0.\n",
    "\n",
    "Leaky ReLU Function\n",
    "Leaky ReLU is an improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area.\n",
    "\n",
    "Parametric ReLU Function\n",
    "Parametric ReLU is another variant of ReLU that aims to solve the problem of gradient’s becoming zero for the left half of the axis.\n",
    "\n",
    "This function provides the slope of the negative part of the function as an argument a. By performing backpropagation, the most appropriate value of a is learnt.\n",
    "\n",
    "Exponential Linear Units (ELUs) Function\n",
    "Exponential Linear Unit, or ELU for short, is also a variant of ReLU that modifies the slope of the negative part of the function.\n",
    "\n",
    "ELU uses a log curve to define the negativ values unlike the leaky ReLU and Parametric ReLU functions with a straight line.\n",
    "\n",
    "Softmax Function\n",
    "Before exploring the ins and outs of the Softmax activation function, we should focus on its building block—the sigmoid/logistic activation function that works on calculating probability values.\n",
    "\n",
    "Swish\n",
    "It is a self-gated activation function developed by researchers at Google.\n",
    "\n",
    "Swish consistently matches or outperforms ReLU activation function on deep networks applied to various challenging domains such as image classification, machine translation etc.\n",
    "\n",
    "Gaussian Error Linear Unit (GELU)\n",
    "The Gaussian Error Linear Unit (GELU) activation function is compatible with BERT, ROBERTa, ALBERT, and other top NLP models. This activation function is motivated by combining properties from dropout, zoneout, and ReLUs.\n",
    "\n",
    "ReLU and dropout together yield a neuron’s output. ReLU does it deterministically by multiplying the input by zero or one (depending upon the input value being positive or negative) and dropout stochastically multiplying by zero.\n",
    "\n",
    "Scaled Exponential Linear Unit (SELU)\n",
    "SELU was defined in self-normalizing networks and takes care of internal normalization which means each layer preserves the mean and variance from the previous layers. SELU enables this normalization by adjusting the mean and variance.\n",
    "\n",
    "SELU has both positive and negative values to shift the mean, which was impossible for ReLU activation function as it cannot output negative values.\n",
    "\n",
    "Gradients can be used to adjust the variance. The activation function needs a region with a gradient larger than one to increase it.\n",
    "\n",
    "3.\n",
    "Explain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?\n",
    "Use a simple perceptron with weights w0, w1, and w2 as −1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0).\n",
    "1.\n",
    "\n",
    "Rosenblatt perceptron is a binary single neuron model. The inputs integration is implemented through the addition of the weighted inputs that have fixed weights obtained during the training stage. If the result of this addition is larger than a given threshold θ the neuron fires. When the neuron fires its output is set to 1, otherwise it’s set to 0.\n",
    "\n",
    "The equation can be re-written as follows including what it’s known as the bias term: This model implements the functioning of a single neuron that can solve linear classification problems through very simple learning algorithms. Rosenblatt Perceptrons are considered as the first generation of neural networks (the network is only compound of one neuron ☺ ). This simple single neuron model has the main limitation of not being able to solve non-linear separable problems. In my next post I will describe how this advantage was overcome and what happens when we have a layer of various perceptrons or try different neuron activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a511b01",
   "metadata": {},
   "outputs": [],
   "source": [
    ". Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98219a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "The solution to this problem is to expand beyond the single-layer architecture by adding an additional layer of units without any direct access to the outside world, known as a hidden layer. This kind of architecture — shown in Figure 4 — is another feed-forward network known as a multilayer perceptron (MLP). image.png It is worth noting that an MLP can have any number of units in its input, hidden and output layers. There can also be any number of hidden layers. The architecture used here is designed specifically for the XOR problem\n",
    "\n",
    "Similar to the classic perceptron, forward propagation begins with the input values and bias unit from the input layer being multiplied by their respective weights, however, in this case there is a weight for each combination of input (including the input layer’s bias unit) and hidden unit (excluding the hidden layer’s bias unit). The products of the input layer values and their respective weights are parsed as input to the non-bias units in the hidden layer. Each non-bias hidden unit invokes an activation function — usually the classic sigmoid function in the case of the XOR problem — to squash the sum of their input values down to a value that falls between 0 and 1 (usually a value very close to either 0 or 1). The outputs of each hidden layer unit, including the bias unit, are then multiplied by another set of respective weights and parsed to an output unit. The output unit also parses the sum of its input values through an activation function — again, the sigmoid function is appropriate here — to return an output value falling between 0 and 1. This is the predicted output.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d79d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Human brains interpret the context of real-world situations in a way that computers can’t. Neural networks were first developed in the 1950s to address this issue. An artificial neural network is an attempt to simulate the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a humanlike manner. ANNs are created by programming regular computers to behave as though they are interconnected brain cells.\n",
    "\n",
    "There are several ways artificial neural networks can be deployed including to classify information, predict outcomes and cluster data. As the networks process and learn from data they can classify a given data set into a predefined class, it can be trained to predict outputs that are expected from a given input and can identify a special feature of data to then classify the data by that special feature. Google uses a 30-layered neural network to power Google Photos as well as to power its “watch next” recommendations for YouTube videos. Facebook uses artificial neural networks for its DeepFace algorithm, which can recognize specific faces with 97% accuracy. It’s also an ANN that powers Skype’s ability to do translations in real-time.\n",
    "\n",
    "Computers have the ability to understand the world around them in a very human-like manner thanks to the power of artificial neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4318f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Explain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584031f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning means to do and adapt the change in itself as and when there is a change in environment. ANN is a complex system or more precisely we can say that it is a complex adaptive system, which can change its internal structure based on the information passing through it.\n",
    "\n",
    "Mathematical Formulation − According to Hebbian learning rule, following is the formula to increase the weight of connection at every time step.\n",
    "\n",
    "Δwji(t)=αxi(t).yj(t) Here, Δwji(t) ⁡= increment by which the weight of connection increases at time step t\n",
    "\n",
    "α = the positive and constant learning rate\n",
    "\n",
    "xi(t) = the input value from pre-synaptic neuron at time step t\n",
    "\n",
    "yi(t) = the output of pre-synaptic neuron at same time step t\n",
    "\n",
    "In neuroscience and computer science, synaptic weight refers to the strength or amplitude of a connection between two nodes, corresponding in biology to the amount of influence the firing of one neuron has on another. The term is typically used in artificial and biological neural network research.\n",
    "\n",
    "In a computational neural network, a vector or set of inputs {\\displaystyle {\\textbf {x}}}{\\textbf {x}} and outputs {\\displaystyle {\\textbf {y}}}{\\textbf {y}}, or pre- and post-synaptic neurons respectively, are interconnected with synaptic weights represented by the matrix {\\displaystyle w}w, where for a linear neuron\n",
    "\n",
    "The synaptic weight is changed by using a learning rule, the most basic of which is Hebb's rule, which is usually stated in biological terms as\n",
    "\n",
    "Neurons that fire together, wire together.\n",
    "\n",
    "Computationally, this means that if a large signal from one of the input neurons results in a large signal from one of the output neurons, then the synaptic weight between those two neurons will increase. The rule is unstable, however, and is typically modified using such variations as Oja's rule, radial basis functions or the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabca625",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Explain, in details, the backpropagation algorithm. What are the limitations of this algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba594e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Back Propagation or back propagation of error is an algorithm for supervised learning of artificial neural networks using gradient descent. It is, though, prominently used to train the multi-layered feedforward neural networks, the main objective of the backpropagation algorithm is to adjust the weights of the neurons in the neural networks, on the basis of the given the error function, to ensure the actual output is closer to the expected result. This is performed in the form of a derivation by applying the chain rule to the error function partial derivative.\n",
    "\n",
    "First introduced in the 1970s as a general optimization method for performing automatic differentiation of complex nested functions, the backpropagation algorithm found its importance in machine learning only after the publication of a paper titled \"Learning Representations by Back-Propagating Errors” by Rumelhart, Hinton & Williams, in 1986. Since then, researchers have been working towards unraveling the backpropagation algorithm to get maximum benefits.\n",
    "\n",
    "Today, some common back propagation algorithm example include deep learning, machine learning, and natural language processing, all of which make use of the algorithm to improve the results delivered for a problem.\n",
    "\n",
    "Now that we comprehend the basics of the backpropagation algorithm, let's move on to understanding how it works.\n",
    "\n",
    "DISADVANTAGES OF BACK PROPAGATION ALGORITHM:\n",
    "\n",
    "Though the advantages of backpropagation outnumber its disadvantages, it is still imperative to highlight these limitations. Therefore, here are the limitations of back propagation algorithms.\n",
    "\n",
    "It relies on input to perform on a specific problem.\n",
    "Sensitive to complex/noisy data.\n",
    "It needs the derivatives of activation functions for the network design time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Describe, in details, the process of adjusting the interconnection weights in a multi-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Error Calculations and Weight Adjustments\n",
    "Output Layer\n",
    "\n",
    "If the actual activation value of the output node, k, is Ok, and the expected target output for node k is tk, the difference between the actual output and the expected output is given by:\n",
    "\n",
    "The error signal for node k in the output layer can be calculated as\n",
    "\n",
    "or\n",
    "\n",
    "where the Ok(1-Ok) term is the derivative of the Sigmoid function.\n",
    "\n",
    "With the delta rule, the change in the weight connecting input node j and output node k is proportional to the error at node k multiplied by the activation of node j.\n",
    "\n",
    "The formulas used to modify the weight, wj,k, between the output node, k, and the node, j is:\n",
    "\n",
    "where is the change in the weight between nodes j and k, lr is the learning rate. The learning rate is a relatively small constant that indicates the relative change in weights. If the learning rate is too low, the network will learn very slowly, and if the learning rate is too high, the network may oscillate around minimum point (refer to Figure 6), overshooting the lowest point with each weight adjustment, but never actually reaching it. Usually the learning rate is very small, with 0.01 not an uncommon number. Some modifications to the Backpropagation algorithm allows the learning rate to decrease from a large value during the learning process. This has many advantages. Since it is assumed that the network initiates at a state that is distant from the optimal set of weights, training will initially be rapid. As learning progresses, the learning rate decreases as it approaches the optimal point in the minima. Slowing the learning process near the optimal point encourages the network to converge to a solution while reducing the possibility of overshooting. If, however, the learning process initiates close to the optimal point, the system may initially oscillate, but this effect is reduced with time as the learning rate decreases.\n",
    "\n",
    "It should also be noted that, in equation (5), the xk variable is the input value to the node k, and is the same value as the output from node j.\n",
    "\n",
    "To improve the process of updating the weights, a modification to equation (5) is made:\n",
    "\n",
    "Here the weight update during the nth iteration is determined by including a momentum term (), which is multiplied to the (n-1)th iteration of the . The introduction of the momentum term is used to accelerate the learning process by \"encouraging\" the weight changes to continue in the same direction with larger steps. Furthermore, the momentum term prevents the learning process from settling in a local minimum. by \"over stepping\" the small \"hill\". Typically, the momentum term has a value between 0 and 1.\n",
    "\n",
    "It should be noted that no matter what modifications are made to the Backpropagation algorithm, such as including the momentum term, there are no guarantees that the network will not settle in a local minimum. Figure 9 Global and Local Minima of Error Function[3]\n",
    "Hidden Layer The error signal for node j in the hidden layer can be calculated as\n",
    "\n",
    "where the Sum term adds the weighted error signal for all nodes, k, in the output layer.As before, the formula to adjust the weight, wi,j, between the input node, i, and the node, j is:\n",
    "\n",
    "Global Error Finally, Backpropagation is derived by assuming that it is desirable to minimize the error on the output nodes over all the patterns presented to the neural network. The following equation is used to calculate the error function, E, for all patterns\n",
    "\n",
    "Ideally, the error function should have a value of zero when the neural network has been correctly trained. This, however, is numerically unrealistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda51df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "9. What are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "Below are the steps involved in Backpropagation:\n",
    "\n",
    "Step – 1: Forward Propagation.\n",
    "Step – 2: Backward Propagation.\n",
    "Step – 3: Putting all the values together and calculating the updated weight value.\n",
    "Multilayer networks solve the classification problem for non linear sets by employing hidden layers, whose neurons are not directly connected to the output. The additional hidden layers can be interpreted geometrically as additional hyper-planes, which enhance the separation capacity of the network.\n",
    "\n",
    "A multilayer feedforward neural network is an interconnection of perceptrons in which data and calculations flow in a single direction, from the input data to the outputs. The number of layers in a neural network is the number of layers of perceptrons.\n",
    "\n",
    "The use of this system can assist patients, both in reaching self-diagnosis decisions and in monitoring their health. This network structure has many advantages for this forecasting context as this structure works well with big data and provides quick predictions after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Write short notes on:\n",
    "    Artificial neuron\n",
    "Multi-layer perceptron\n",
    "Deep learning\n",
    "Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf88c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Artificial neuron\n",
    "An artificial neuron is a connection point in an artificial neural network. Artificial neural networks, like the human body's biological neural network, have a layered architecture and each network node (connection point) has the capability to process input and forward output to other nodes in the network. In both artificial and biological architectures, the nodes are called neurons and the connections are characterized by synaptic weights, which represent the significance of the connection. As new data is received and processed, the synaptic weights change and this is how learning occurs.\n",
    "\n",
    "Artificial neurons are modeled after the hierarchical arrangement of neurons in biological sensory systems. In the visual system, for example, light input passes through neurons in successive layers of the retina before being passed to neurons in the thalamus of the brain and then on to neurons in the brain's visual cortex. As the neurons pass signals through an increasing number of layers, the brain progressively extracts more information until it is confident it can identify what the person is seeing. In artificial intelligence, this fine tuning process is known as deep learning.\n",
    "\n",
    "In both artificial and biological networks, when neurons process the input they receive, they decide whether the output should be passed on to the next layer as input. The decision of whether or not to send information on is called bias and it’s determined by an activation function built into the system. For example, an artificial neuron may only pass an output signal on to the next layer if its inputs (which are actually voltages) sum to a value above some particular threshold value. Because activation functions can either be linear or non-linear, neurons will often have a wide range of convergence and divergence. Divergence is the ability for one neuron to communicate with many other neurons in the network and convergence is the ability for one neuron to receive input from many other neurons in the network.\n",
    "\n",
    "Multi-layer perceptron\n",
    "To be accurate a fully connected Multi-Layered Neural Network is known as Multi-Layer Perceptron. A Multi-Layered Neural Network consists of multiple layers of artificial neurons or nodes. Unlike Single-Layer Neural Network, in recent times most of the networks have Multi-Layered Neural Network. The following diagram is a visualization of a multi-layer neural network.\n",
    "\n",
    "image.pngExplanation:\n",
    "\n",
    "Here the nodes marked as “1” are known as bias units. The leftmost layer or Layer 1 is the input layer, the middle layer or Layer 2 is the hidden layer and the rightmost layer or Layer 3 is the output layer. It can say that the above diagram has 3 input units (leaving the bias unit), 1 output unit, and 3 hidden units. A Multi-layered Neural Network is the typical example of the Feed Forward Neural Network. The number of neurons and the number of layers consists of the hyperparameters of Neural Networks which need tuning. In order to find ideal values for the hyperparameters, one must use some cross-validation techniques. Using the Back-Propagation technique, weight adjustment training is carried out.\n",
    "\n",
    "3.Deep learning\n",
    "\n",
    "Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost. It is the key to voice control in consumer devices like phones, tablets, TVs, and hands-free speakers. Deep learning is getting lots of attention lately and for good reason. It’s achieving results that were not possible before.\n",
    "\n",
    "In deep learning, a computer model learns to perform classification tasks directly from images, text, or sound. Deep learning models can achieve state-of-the-art accuracy, sometimes exceeding human-level performance. Models are trained by using a large set of labeled data and neural network architectures that contain many layers.\n",
    "\n",
    "Learning rate\n",
    "The amount that the weights are updated during training is referred to as the step size or the “learning rate.” Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.\n",
    "\n",
    "There are multiple ways to select a good starting point for the learning rate. A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. We might start with a large value like 0.1, then try exponentially lower values: 0.01, 0.001, etc.\n",
    "\n",
    "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dcbb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Write the difference between:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3627930",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation function vs threshold function\n",
    "The activation function is a mathematical “gate” in between the input feeding the current neuron and its output going to the next layer. They basically decide whether the neuron should be activated or not.\n",
    "\n",
    "A threshold function is a Boolean function that determines whether a value equality of its inputs exceeded a certain threshold. A device that implements such logic is known as a threshold gate.\n",
    "\n",
    "Binary Step Activation Function. Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated. In the above graph, the threshold is zero.\n",
    "\n",
    "Step function vs sigmoid function\n",
    "The (Heaviside) step function is typically only useful within single-layer perceptrons, an early type of neural networks that can be used for classification in cases where the input data is linearly separable.\n",
    "\n",
    "However, multi-layer neural networks or multi-layer perceptrons are of more interest because they are general function approximators and they are able to distinguish data that is not linearly separable.\n",
    "\n",
    "Multi-layer perceptrons are trained using backpropapagation. A requirement for backpropagation is a differentiable activation function. That's because backpropagation uses gradient descent on this function to update the network weights.\n",
    "\n",
    "The Heaviside step function is non-differentiable at x = 0 and its derivative is 0 elsewhere. This means gradient descent won't be able to make progress in updating the weights and backpropagation will fail.\n",
    "\n",
    "The sigmoid or logistic function does not have this shortcoming and this explains its usefulness as an activation function within the field of neural networks.\n",
    "\n",
    "Single layer vs multi-layer perceptron\n",
    "Single layer perceptron is a simple Neural Network which contains only one layer. The single layer computation of perceptron is the calculation of sum of input vector with the value multiplied by corresponding vector weight. The displayed output value will be the input of an activation function. Single Layer Perceptron has just two layers of input and output. It only has single layer hence the name single layer perceptron. It does not contain Hidden Layers as that of Multilayer perceptron.Input nodes are connected fully to a node or multiple nodes in the next layer. A node in the next layer takes a weighted sum of all its inputs\n",
    "\n",
    "A multilayer perceptron is a type of feed-forward artificial neural network that generates a set of outputs from a set of inputs. An MLP is a neural network connecting multiple layers in a directed graph, which means that the signal path through the nodes only goes one way. The MLP network consists of input, output, and hidden layers. Each hidden layer consists of numerous perceptron’s which are called hidden layers or hidden unit.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
