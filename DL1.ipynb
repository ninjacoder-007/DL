{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the function of a summation junction of a neuron? What is threshold activation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "summation, in physiology, the additive effect of several electrical impulses on a neuromuscular junction, the junction between a nerve cell and a muscle cell. Individually the stimuli cannot evoke a response, but collectively they can generate a response.\n",
    "\n",
    "The axon hillock makes up the summation zone of the neuron which adds together the nerve impulses and determines whether to send the impulse further along the neuron.\n",
    "\n",
    "Binary Step Activation Function. Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated. In the above graph, the threshold is zero\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is a step function? What is the difference of step function with threshold function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A step function is a function like that used by the original Perceptron. The output is a certain value, A1, if the input sum is above a certain threshold and A0 if the input sum is below a certain threshold. The values used by the Perceptron were A1 = 1 and A0 = 0.\n",
    "Step Function is one of the simplest kind of activation functions. In this, we consider a threshold value and if the value of net input say y is greater than the threshold then the neuron is activated. Mathematically, Given below is the graphical representation of step function.\n",
    "\n",
    "Binary Step Activation Function. Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated. In the above graph, the threshold is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb398bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Explain the McCulloch–Pitts model of neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fe831",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here is a graphical representation of the McCulloch-Pitts model \n",
    "\n",
    "The McCulloch-Pitts model was an extremely simple artificial neuron. The inputs could be either a zero or a one. And the output was a zero or a one. And each input could be either excitatory or inhibitory.\n",
    "\n",
    "Now the whole point was to sum the inputs. If an input is one, and is excitatory in nature, it added one. If it was one, and was inhibitory, it subtracted one from the sum. This is done for all inputs, and a final sum is calculated.\n",
    "\n",
    "Now, if this final sum is less than some value (which you decide, say T), then the output is zero. Otherwise, the output is a one.\n",
    "\n",
    "This is simplified model of real neurons, known as Threshold Logic Unit. A set of synapsesc (i.e connections) brings the activations from the other neurons. A processing unit sums the inputs, the applies the non-linear activation funcation (i.e threshold / transfer function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain the ADALINE network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network uses memistors. It was developed by Professor Bernard Widrow and his doctorate student Ted Hoff at Stanford University in 1960. It is based on the McCulloch–Pitts neuron. It consists of a weight, a bias and a summation function.\n",
    "\n",
    "The difference between Adaline and the standard (McCulloch–Pitts) perceptron is that in the learning phase, the weights are adjusted according to the weighted sum of the inputs (the net). In the standard perceptron, the net is passed to the activation (transfer) function and the function's output is used for adjusting the weights.\n",
    "\n",
    "A multilayer network of ADALINE units is known as a MADALINE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is the constraint of a simple perceptron? Why it may fail with a real-world data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ee21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The perceptron is the building block of artificial neural networks, it is a simplified model of the biological neurons in our brain. A perceptron is the simplest neural network, one that is comprised of just one neuron.There are about 1,000 to 10,000 connections that are formed by other neurons to these dendrites.\n",
    "\n",
    "Why it may fail\n",
    "Perceptron networks have several limitations. First, the output values of a perceptron can take on only one of two values (0 or 1) due to the hard-limit transfer function. Second, perceptrons can only classify linearly separable sets of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What is linearly inseparable problem? What is the role of the hidden layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584402d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A set of input vectors (or a training set) will be said to be linearly non-separable if no hyperplane exists such that each vector lies on the pre-assigned side of the hyperplane.\n",
    "\n",
    "Clearly not all decision problems are linearly separable: they cannot be solved using a linear decision boundary. Problems like these are termed linearly inseparable. XOR is a linearly inseparable problem.\n",
    "\n",
    "Linearly separable data is data that if graphed in two dimensions, can be separated by a straight line. Here's an example: This data is linearly separable because there is a line (actually many lines) from lower left to upper right that separates the red and blue classes\n",
    "Hidden layers, simply put, are layers of mathematical functions each designed to produce an output specific to an intended result. For example, some forms of hidden layers are known as squashing functions. These functions are particularly useful when the intended output of the algorithm is a probability because they take an input and produce an output value between 0 and 1, the range for defining probability.\n",
    "\n",
    "Hidden layers allow for the function of a neural network to be broken down into specific transformations of the data. Each hidden layer function is specialized to produce a defined output. For example, a hidden layer functions that are used to identify human eyes and ears may be used in conjunction by subsequent layers to identify faces in images. While the functions to identify eyes alone are not enough to independently recognize objects, they can function jointly within a neural network.\n",
    "\n",
    "In artificial neural networks, hidden layers are required if and only if the data must be separated non-linearly. Looking at figure 2, it seems that the classes must be non-linearly separated. A single line will not work. As a result, we must use hidden layers in order to get the best decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475eeae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Explain XOR problem in case of a simple perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17392b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Perceptron for XOR:\n",
    "\n",
    "XOR is where if one is 1 and other is 0 but not both.\n",
    "\n",
    "Need: 1.w1 + 0.w2 cause a fire, i.e. >= t 0.w1 + 1.w2 >= t 0.w1 + 0.w2 doesn't fire, i.e. < t 1.w1 + 1.w2 also doesn't fire, < t\n",
    "\n",
    "w1 >= t w2 >= t 0 < t w1+w2 < t Contradiction.\n",
    "\n",
    "Note: We need all 4 inequalities for the contradiction. If weights negative, e.g. weights = -4 and t = -5, then weights can be greater than t yet adding them is less than t, but t > 0 stops this.\n",
    "\n",
    "A \"single-layer\" perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight line to separate the points (0,0),(1,1) from the points (0,1),(1,0).\n",
    "\n",
    "Led to invention of multi-layer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Design a multi-layer perceptron to implement A XOR B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "The solution to this problem is to expand beyond the single-layer architecture by adding an additional layer of units without any direct access to the outside world, known as a hidden layer. This kind of architecture — shown in Figure 4 — is another feed-forward network known as a multilayer perceptron (MLP). image.png It is worth noting that an MLP can have any number of units in its input, hidden and output layers. There can also be any number of hidden layers. The architecture used here is designed specifically for the XOR problem\n",
    "\n",
    "Similar to the classic perceptron, forward propagation begins with the input values and bias unit from the input layer being multiplied by their respective weights, however, in this case there is a weight for each combination of input (including the input layer’s bias unit) and hidden unit (excluding the hidden layer’s bias unit). The products of the input layer values and their respective weights are parsed as input to the non-bias units in the hidden layer. Each non-bias hidden unit invokes an activation function — usually the classic sigmoid function in the case of the XOR problem — to squash the sum of their input values down to a value that falls between 0 and 1 (usually a value very close to either 0 or 1). The outputs of each hidden layer unit, including the bias unit, are then multiplied by another set of respective weights and parsed to an output unit. The output unit also parses the sum of its input values through an activation function — again, the sigmoid function is appropriate here — to return an output value falling between 0 and 1. This is the predicted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Explain the single-layer feed forward architecture of ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Single Layer Feed Forward Network\n",
    "In this type of network, we have only two layers, i.e. input layer and output layer but the input layer does not count because no computation is performed in this layer.\n",
    "\n",
    "Output Layer is formed when different weights are applied on input nodes and the cumulative effect per node is taken.\n",
    "\n",
    "After this, the neurons collectively give the output layer to compute the output signals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46fe389",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Explain the competitive network architecture of ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network. It is well suited to finding clusters within data. Models and algorithms based on the principle of competitive learning include vector quantization and self-organizing maps\n",
    "\n",
    "Architecture and implementation\n",
    "\n",
    "\n",
    "Competitive Learning is usually implemented with Neural Networks that contain a hidden layer which is commonly known as “competitive layer”.[6] Every competitive neuron is described by a vector of weights\n",
    "\n",
    "For every input vector, the competitive neurons “compete” with each other to see which one of them is the most similar to that particular input vector. The winner neuron m sets its output {\\displaystyle o{m}=1}{\\displaystyle o{m}=1} and all the other competitive neurons set their output {\\displaystyle o{i}=0,i=1,..,M,i\\neq m}o{i}=0,i=1,..,M,i\\neq m.\n",
    "\n",
    "Usually, in order to measure similarity the inverse of the Euclidean distance is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a16ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Below are the steps involved in Backpropagation:\n",
    "\n",
    "Step – 1: Forward Propagation.\n",
    "\n",
    "Step – 2: Backward Propagation.\n",
    "\n",
    "Step – 3: Putting all the values together and calculating the updated weight value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23400fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the advantages and disadvantages of neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are some advantages of Artificial Neural Networks ( ANN)\n",
    "\n",
    "Storing information on the entire network: Information such as in traditional programming is stored on the entire network, not on a database. The disappearance of a few pieces of information in one place does not restrict the network from functioning.\n",
    "\n",
    "The ability to work with inadequate knowledge: After ANN training, the data may produce output even with incomplete information. The lack of performance here depends on the importance of the missing information.\n",
    "\n",
    "It has fault tolerance: Corruption of one or more cells of ANN does not prevent it from generating output. This feature makes the networks fault-tolerant.\n",
    "\n",
    "Having a distributed memory: For ANN to be able to learn, it is necessary to determine the examples and to teach the network according to the desired output by showing these examples to the network. The network's progress is directly proportional to the selected instances, and if the event can not be shown to the network in all its aspects, the network can produce incorrect output\n",
    "\n",
    "Gradual corruption: A network slows over time and undergoes relative degradation. The network problem does not immediately corrode.\n",
    "\n",
    "Ability to train machine: Artificial neural networks learn events and make decisions by commenting on similar events.\n",
    "\n",
    "Parallel processing ability: Artificial neural networks have numerical strength that can perform more than one job at the same time.\n",
    "Disadvantages of Artificial Neural Networks (ANN)\n",
    "\n",
    "Hardware dependence: Artificial neural networks require processors with parallel processing power, by their structure. For this reason, the realization of the equipment is dependent.\n",
    "\n",
    "Unexplained functioning of the network: This is the most important problem of ANN. When ANN gives a probing solution, it does not give a clue as to why and how. This reduces trust in the network.\n",
    "\n",
    "Assurance of proper network structure: There is no specific rule for determining the structure of artificial neural networks. The appropriate network structure is achieved through experience and trial and error.\n",
    "\n",
    "The difficulty of showing the problem to the network: ANNs can work with numerical information. Problems have to be translated into numerical values before being introduced to ANN. The display mechanism to be determined here will directly influence the performance of the network. This depends on the user's ability.\n",
    "\n",
    "The duration of the network is unknown: The network is reduced to a certain value of the error on the sample means that the training has been completed. This value does not give us optimum results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaad0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "13. Write short notes on any two of the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b4bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Biological neuron\n",
    "\n",
    "ReLU function\n",
    "Single-layer feed forward ANN\n",
    "Gradient descent\n",
    "Recurrent networks\n",
    "Biological neuron\n",
    "Biological neurons, consisting of a cell body, axons, dendrites and synapses, are able to process and transmit neural activation.\n",
    "\n",
    "Biological neuron models, also known as a spiking neuron models,[1] are mathematical descriptions of the properties of certain cells in the nervous system that generate sharp electrical potentials across their cell membrane, roughly one millisecond in duration, called action potentials or spikes (Fig. 2). Since spikes are transmitted along the axon and synapses from the sending neuron to many other neurons, spiking neurons are considered to be a major information processing unit of the nervous system. Spiking neuron models can be divided into different categories: the most detailed mathematical models are biophysical neuron models (also called Hodgkin-Huxley models) that describe the membrane voltage as a function of the input current and the activation of ion channels. Mathematically simpler are integrate-and-fire models that describe the membrane voltage as a function of the input current and predict the spike times without a description of the biophysical processes that shape the time course of an action potential. Even more abstract models only predict output spikes (but not membrane voltage) as a function of the stimulation where the stimulation can occur through sensory input or\n",
    "    pharmacologically. This article provides a short overview of different spiking neuron models and links, whenever possible to experimental phenomena. It includes deterministic and probabilistic models.\n",
    "\n",
    "Recurrent networks\n",
    "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition[4] or speech recognition. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.\n",
    "\n",
    "The term “recurrent neural network” is used to refer to the class of networks with an infinite impulse response, whereas “convolutional neural network” refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.[8] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\n",
    "\n",
    "Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).\n",
    "\n",
    "In typical libraries like PyTorch, just-in-time compilation plays an important role in efficiently implementing recurrent neural networks.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
